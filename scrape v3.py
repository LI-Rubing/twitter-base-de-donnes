import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime, timedelta
import os
import time
import json
import re
import random
import subprocess
from fake_useragent import UserAgent

# Chrome ç”¨æˆ·æ•°æ®ç›®å½•
BASE_USER_DATA_DIR = "/home/user/.config/google-chrome/Default"

class RequestLimiter:
    def __init__(self):
        self.last_request_time = 0
        self.request_count = 0
        self.reset_time = time.time() + 3600  # æ¯å°æ—¶é‡ç½®è®¡æ•°å™¨
        
    def check_limit(self):
        current = time.time()
        # æ¯åˆ†é’Ÿä¸è¶…è¿‡60æ¬¡è¯·æ±‚
        if self.request_count > 60 and current < self.last_request_time + 60:
            wait = (self.last_request_time + 20) - current
            print(f"âš ï¸ è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œç­‰å¾… {wait:.1f}ç§’")
            time.sleep(wait + random.uniform(1, 3))
            self.request_count = 0
        
        # æ¯å°æ—¶é‡ç½®è®¡æ•°å™¨
        if current > self.reset_time:
            self.reset_time = current + 3600
            self.request_count = 0
            
        self.last_request_time = current
        self.request_count += 1

def random_delay(min_sec=1.5, max_sec=5.0):
    """éšæœºå»¶è¿Ÿ"""
    delay = random.uniform(min_sec, max_sec)
    if random.random() < 0.02:  # 6%æ¦‚ç‡æ’å…¥é•¿å»¶è¿Ÿ
        delay += random.uniform(7, 10)
    time.sleep(delay)

def get_random_useragent():
    """åŠ¨æ€User-Agent"""
    ua = UserAgent()
    return ua.chrome

def create_stealth_driver():
    # æ¸…ç†å¯èƒ½çš„ä¸´æ—¶æ–‡ä»¶
    #os.system("pkill -f chrome")  # ç¡®ä¿æ²¡æœ‰æ®‹ç•™çš„Chromeè¿›ç¨‹
    os.system("rm -rf /tmp/chrome-tmp")  # æ¸…ç†ç¼“å­˜ç›®å½•
    
    retries = 3
    for attempt in range(retries):
        try:
            # æ¯æ¬¡å°è¯•éƒ½åˆ›å»ºæ–°çš„ChromeOptionså¯¹è±¡
            options = uc.ChromeOptions()
            
            # åŸºæœ¬é…ç½®
            options.add_argument("--start-maximized")
            #options.add_argument(f"--user-data-dir={BASE_USER_DATA_DIR}")
            #options.add_argument("--profile-directory=Default")
            options.add_argument("--disk-cache-dir=/tmp/chrome-tmp")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            
            # è·å–Chromeç‰ˆæœ¬
            try:
                chrome_version = subprocess.getoutput("/usr/bin/google-chrome --version").split()[2].split('.')[0]
            except:
                chrome_version = 137  # é»˜è®¤ç‰ˆæœ¬

            print("å°è¯•å¯åŠ¨ Chrome æµè§ˆå™¨...")
            
            # æœ€å°åŒ–åˆå§‹åŒ–é…ç½®
            driver = uc.Chrome(
                options=options,
                browser_executable_path="/usr/bin/google-chrome",
                version_main=int(chrome_version),
            )
            print("Chrome å·²æˆåŠŸå¯åŠ¨")
            # éªŒè¯è¿æ¥
            driver.get("about:blank")
            time.sleep(1)
            return driver
            
        except Exception as e:
            print(f"åˆå§‹åŒ–å¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {str(e)[:200]}")
            os.system("pkill -f chrome")
            time.sleep(2)
            continue

    raise RuntimeError("æ— æ³•åˆå§‹åŒ–Chromeé©±åŠ¨ï¼Œè¯·æ£€æŸ¥æµè§ˆå™¨å®‰è£…")

def handle_rate_limit(driver):
    """å¤„ç†è¢«å°ç¦çš„æƒ…å†µ"""
    wait_times = [300, 600, 1800]  # 5åˆ†é’Ÿ, 10åˆ†é’Ÿ, 30åˆ†é’Ÿ
    for wait in wait_times:
        print(f"ğŸš¨ å¯èƒ½è¢«é™æµï¼Œç­‰å¾… {wait//60} åˆ†é’Ÿ...")
        time.sleep(wait)
        
        try:
            driver.get("https://x.com/about")
            if "About" in driver.title:
                print("âœ… é™åˆ¶å·²è§£é™¤ï¼Œç»§ç»­çˆ¬å–")
                return True
        except:
            continue
            
    return False

def is_within_date_range(tweet_date, start_date, end_date):
    """æ£€æŸ¥æ¨æ–‡æ—¥æœŸæ˜¯å¦åœ¨æŒ‡å®šèŒƒå›´å†…"""
    return start_date <= tweet_date <= end_date

def extract_tweet_id(url):
    """ä»URLä¸­æå–æ¨æ–‡ID"""
    match = re.search(r'/status/(\d+)', url)
    return match.group(1) if match else None

def main_scraper(tag, max_scrolls, driver, start_date, end_date, limiter):
    print(f"å¼€å§‹çˆ¬å– #{tag} ç›¸å…³çš„æ¨æ–‡ï¼Œæ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}...")
    search_url = f"https://x.com/search?q={tag}&src=typed_query"

    retries = 2
    for attempt in range(retries):
        try:
            limiter.check_limit()
            driver.get(search_url)
            WebDriverWait(driver, random.randint(15, 25)).until(
                EC.presence_of_element_located((By.XPATH, "//article[@data-testid='tweet']"))
            )
            random_delay()
            break
        except Exception as e:
            if attempt < retries - 1:
                print(f"é¡µé¢åŠ è½½å¤±è´¥ï¼Œé‡è¯•ä¸­ ({attempt + 1}): {str(e)[:100]}...")
                random_delay(3, 8)
                driver.refresh()
            else:
                print("é¡µé¢åŠ è½½å¤±è´¥ï¼Œè·³è¿‡")
                return []

    seen = set()
    all_tweets = []
    filtered_tweets = []
    last_count = 0
    stable_scrolls = 0

    for scroll_idx in range(max_scrolls):
        limiter.check_limit()
        
        # éšæœºæ»šåŠ¨è¡Œä¸º
        scroll_height = random.randint(1500, 2000)
        driver.execute_script(f"window.scrollBy(0, {scroll_height});")
        random_delay(0.4, 1.2)

        tweet_cards = driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
        if not tweet_cards:
            print("æ²¡æœ‰æ‰¾åˆ°æ›´å¤šæ¨æ–‡ï¼Œåœæ­¢æ»šåŠ¨")
            break

        for card in tweet_cards:
            try:
                limiter.check_limit()
                
                time_element = card.find_element(By.TAG_NAME, "time")
                timestamp = time_element.get_attribute("datetime")
                tweet_date = datetime.strptime(timestamp, "%Y-%m-%dT%H:%M:%S.000Z").date()

                tweet_url = ""
                links = card.find_elements(By.TAG_NAME, "a")
                for link in links:
                    href = link.get_attribute("href")
                    if href and re.search(r"https://x\.com/[^/]+/status/\d+$", href):
                        tweet_url = href
                        break
                
                tweet_id = extract_tweet_id(tweet_url)
                if not tweet_id or tweet_id in seen:
                    continue

                username = card.find_element(By.XPATH, ".//div[@dir='ltr']//span").text
                try:
                    content = card.find_element(By.XPATH, ".//div[@lang]").text
                except Exception as e:
                    content = ""
                    print(f"æå–å†…å®¹å‡ºé”™: {str(e)[:100]}...")

                tweet_data = {
                    "id": tweet_id,
                    "username": username,
                    "timestamp": timestamp,
                    "date": str(tweet_date),
                    "content": content,
                    "url": tweet_url,
                    "replies": []
                }

                all_tweets.append(tweet_data)
                seen.add(tweet_id)

                if is_within_date_range(tweet_date, start_date, end_date):
                    filtered_tweets.append(tweet_data)

            except Exception as e:
                print(f"å¤„ç†æ¨æ–‡æ—¶å‡ºé”™: {str(e)[:100]}...")
                continue

        print(f"æ»šåŠ¨ {scroll_idx + 1}/{max_scrolls}, æ€»æ¨æ–‡æ•°: {len(all_tweets)}, ç¬¦åˆæ—¥æœŸèŒƒå›´çš„æ¨æ–‡: {len(filtered_tweets)}")

        if len(all_tweets) == last_count:
            stable_scrolls += 1
            if stable_scrolls >= 3:  # è¿ç»­3æ¬¡æ— æ–°å†…å®¹åˆ™åœæ­¢
                print("è¿ç»­3æ¬¡æ»šåŠ¨æ²¡æœ‰å‘ç°æ–°å†…å®¹ï¼Œåœæ­¢æ»šåŠ¨")
                break
        else:
            stable_scrolls = 0
        last_count = len(all_tweets)

        # éšæœºé˜…è¯»æ¨¡å¼
        if random.random() < 0.2:
            read_time = random.uniform(3, 4)
            print(f"æ¨¡æ‹Ÿé˜…è¯»è¡Œä¸ºï¼Œæš‚åœ {read_time:.1f}ç§’...")
            time.sleep(read_time)

    return filtered_tweets

class ReplyScraper:
    def __init__(self, driver, limiter):
        self.driver = driver
        self.limiter = limiter
        self.seen_ids = set()  # å…¨å±€å·²çˆ¬å–çš„æ¨æ–‡IDè®°å½•
        self.max_depth = 0
        self.current_depth = 0
    
    def fetch_replies(self, tweet, max_depth=2, current_depth=1, max_replies_per_level=5 ):
        if current_depth > max_depth:
            return tweet
            
        try:
            self.limiter.check_limit()
            print(f"\n=== å¼€å§‹è·å–å›å¤ (æ·±åº¦ {current_depth}/{max_depth}) ===")
            print(f"çˆ¶æ¨æ–‡URL: {tweet['url']}")
            
            tweet_id = tweet.get('id') or extract_tweet_id(tweet['url'])
            if not tweet_id:
                print("æ— æ³•è·å–æ¨æ–‡IDï¼Œè·³è¿‡")
                return tweet
                
            if tweet_id in self.seen_ids:
                print(f"æ¨æ–‡ {tweet_id} å·²ç»å¤„ç†è¿‡ï¼Œè·³è¿‡")
                return tweet
                
            self.seen_ids.add(tweet_id)
            
            # åŠ¨æ€ä¿®æ”¹User-Agent
            self.driver.execute_cdp_cmd("Network.setUserAgentOverride", {
                "userAgent": get_random_useragent()
            })
            
            self.driver.get(tweet["url"])
            WebDriverWait(self.driver, random.randint(15, 25)).until(
                EC.presence_of_element_located((By.XPATH, "//article[@data-testid='tweet']"))
            )
            random_delay()

            # è·å–å½“å‰é¡µé¢çš„æ‰€æœ‰å›å¤
            replies = self._extract_replies_from_page(current_depth)
            print(f"åˆæ­¥æå– {len(replies)} æ¡å›å¤ (æ·±åº¦ {current_depth})")
            
            # å¦‚æœå½“å‰æ˜¯ç¬¬ä¸€å±‚ï¼Œå¯èƒ½éœ€è¦å¤„ç†"æŸ¥çœ‹æ›´å¤šå›å¤"çš„æƒ…å†µ
            if current_depth == 0 and len(replies) < 3:
                replies.extend(self._check_for_more_replies(current_depth))
                print(f"æŸ¥çœ‹æ›´å¤šå›å¤åï¼Œå…± {len(replies)} æ¡å›å¤")
            
            # é™åˆ¶æ¯å±‚çš„å›å¤æ•°é‡
            if len(replies) > max_replies_per_level:
                print(f"ä» {len(replies)} æ¡å›å¤ä¸­ç­›é€‰å‰ {max_replies_per_level} æ¡è¿›è¡Œæ·±å±‚çˆ¬å–")
                replies_to_process = replies[:max_replies_per_level]
            else:
                replies_to_process = replies
                
            # é€’å½’è·å–æ·±å±‚å›å¤
            if current_depth < max_depth:
                for i, reply in enumerate(replies_to_process):
                    print(f"\nå¤„ç†ç¬¬ {i + 1}/{len(replies_to_process)} æ¡å›å¤ | URL: {reply['url']}")
                    reply = self.fetch_replies(
                        reply,
                        max_depth,
                        current_depth + 1,
                        max_replies_per_level
                    )
                    
                    if (i + 1) % 15 == 0:
                        sleep_time = random.uniform(3, )
                        print(f"å¤„ç† {i + 1} æ¡åæš‚åœ {sleep_time:.1f} ç§’...")
                        time.sleep(sleep_time)
            
            tweet["replies"] = replies
            print(f"\n=== å®Œæˆè·å–å›å¤ (æ·±åº¦ {current_depth}/{max_depth}) ===")
            return tweet

        except Exception as e:
            if "rate limit" in str(e).lower():
                if not handle_rate_limit(self.driver):
                    raise Exception("æ— æ³•è§£é™¤é™æµï¼Œè¯·æ›´æ¢IPæˆ–ç¨åå†è¯•")
                return self.fetch_replies(tweet, max_depth, current_depth, max_replies_per_level)
            
            print(f"\n!!! è·å–å›å¤å¤±è´¥ (æ·±åº¦ {current_depth}/{max_depth}) !!!")
            print(f"é”™è¯¯: {str(e)[:200]}...")
            tweet["replies"] = []
            return tweet
    
    def _extract_replies_from_page(self, current_depth):
        """ä»å½“å‰é¡µé¢æå–å›å¤"""
        replies = []
        seen_in_this_page = set()
        
        # æ ¹æ®å½“å‰æ·±åº¦å†³å®šè¦å¿½ç•¥çš„å…ƒç´ æ•°é‡
        
        ignore_count = current_depth
        
        for scroll_attempt in range(20):
            self.limiter.check_limit()
            
            cards = self.driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
            valid_cards = cards[ignore_count:] if len(cards) > ignore_count else []
            
            print(f"æ»šåŠ¨ {scroll_attempt}: æ‰¾åˆ° {len(valid_cards)} ä¸ªæœ‰æ•ˆæ¨æ–‡å¡ç‰‡ (å¿½ç•¥å‰{ignore_count}ä¸ª)")
            
            new_replies = []
            for card in valid_cards:
                tweet_data = self._extract_tweet_data(card)
                if not tweet_data:
                    continue
                    
                tweet_id = tweet_data['id']
                if tweet_id in self.seen_ids or tweet_id in seen_in_this_page:
                    continue
                    
                new_replies.append(tweet_data)
                seen_in_this_page.add(tweet_id)
            
            if new_replies:
                print(f"å‘ç° {len(new_replies)} æ¡æ–°å›å¤")
                replies.extend(new_replies)
            else:
                print("æ²¡æœ‰å‘ç°æ–°å›å¤")
                
            # éšæœºæ»šåŠ¨
            scroll_height = random.randint(1500, 2000)
            self.driver.execute_script(f"window.scrollBy(0, {scroll_height});")
            random_delay(0.5, 1.5)
            
            # å¦‚æœè¿ç»­3æ¬¡æ»šåŠ¨æ²¡æœ‰æ–°å†…å®¹ï¼Œåœæ­¢
            if scroll_attempt >= 2 and len(new_replies) == 0:
                break
                
        return replies
    
    def _check_for_more_replies(self, current_depth):
        """æ£€æŸ¥æ˜¯å¦æœ‰'æŸ¥çœ‹æ›´å¤šå›å¤'çš„æŒ‰é’®"""
        more_replies = []
        try:
            more_buttons = self.driver.find_elements(By.XPATH, "//div[@role='button' and contains(., 'æŸ¥çœ‹æ›´å¤šå›å¤')]")
            if more_buttons:
                print(f"å‘ç° {len(more_buttons)} ä¸ª'æŸ¥çœ‹æ›´å¤šå›å¤'æŒ‰é’®")
                for btn in more_buttons[:2]:  # æœ€å¤šç‚¹å‡»å‰ä¸¤ä¸ª
                    try:
                        self.limiter.check_limit()
                        btn.click()
                        random_delay(2, 3)
                        more_replies = self._extract_replies_from_page(current_depth)
                        print(f"ä»'æŸ¥çœ‹æ›´å¤šå›å¤'ä¸­è·å–äº† {len(more_replies)} æ¡å›å¤")
                    except Exception as e:
                        print(f"ç‚¹å‡»'æŸ¥çœ‹æ›´å¤šå›å¤'å¤±è´¥: {str(e)[:100]}...")
                        continue
        except Exception as e:
            print(f"æŸ¥æ‰¾'æŸ¥çœ‹æ›´å¤šå›å¤'æŒ‰é’®å¤±è´¥: {str(e)[:100]}...")
            
        return more_replies
    
    def _extract_tweet_data(self, card):
        """ä»å¡ç‰‡ä¸­æå–æ¨æ–‡æ•°æ®"""
        try:
            username_elem = card.find_element(By.XPATH, ".//div[@data-testid='User-Name']//a")
            username = username_elem.get_attribute("href").split("/")[-1]
            
            content_elem = card.find_elements(By.XPATH, ".//div[@data-testid='tweetText']")
            content = content_elem[0].text if content_elem else ""
            
            time_elem = card.find_elements(By.TAG_NAME, "time")
            if not time_elem:
                return None
                
            timestamp = time_elem[0].get_attribute("datetime")
            
            links = card.find_elements(By.XPATH, ".//a[contains(@href, '/status/')]")
            status_id = None
            for link in links:
                href = link.get_attribute("href")
                match = re.search(r'/status/(\d+)', href)
                if match:
                    status_id = match.group(1)
                    break
            
            if not status_id:
                return None
                
            return {
                "id": status_id,
                "username": username,
                "timestamp": timestamp,
                "date": str(datetime.strptime(timestamp, "%Y-%m-%dT%H:%M:%S.000Z").date()),
                "content": content,
                "url": f"https://x.com/{username}/status/{status_id}",
                "replies": []
            }
        except Exception as e:
            print(f"æå–æ¨æ–‡æ•°æ®å‡ºé”™: {str(e)[:100]}...")
            return None

if __name__ == "__main__":
    os.makedirs("/tmp/chrome-tmp", exist_ok=True)
    
    tag_to_search = "dog"
    max_scrolls = 5
    start_date = datetime.strptime("2025-05-01", "%Y-%m-%d").date()
    end_date = datetime.strptime("2025-06-01", "%Y-%m-%d").date()
    
    driver = create_stealth_driver()
    limiter = RequestLimiter()

    try:
        filtered_tweets = main_scraper(
            tag=tag_to_search,
            max_scrolls=max_scrolls,
            driver=driver,
            start_date=start_date,
            end_date=end_date,
            limiter=limiter
        )
        
        print(f"ä¸»çˆ¬å–å®Œæˆï¼Œæ‰¾åˆ° {len(filtered_tweets)} æ¡ç¬¦åˆæ—¥æœŸèŒƒå›´çš„æ¨æ–‡")
        
        scraper = ReplyScraper(driver, limiter)
        tweets_with_replies = []
        
        for i, tweet in enumerate(filtered_tweets):
            full_tweet = scraper.fetch_replies(tweet)
            tweets_with_replies.append(full_tweet)
            print(f"è¿›åº¦: {i+1}/{len(filtered_tweets)}")
            
            if (i + 1) % 10 == 0:
                sleep_time = random.uniform(10, 30)
                print(f"æ‰¹é‡å¤„ç†æš‚åœ {sleep_time:.1f} ç§’...")
                time.sleep(sleep_time)
                
            # æ¯å¤„ç†10ä¸ªä¸»æ¨æ–‡æ›´æ¢User-Agent
            if (i + 1) % 10 == 0:
                driver.execute_cdp_cmd("Network.setUserAgentOverride", {
                    "userAgent": get_random_useragent()
                })

        output_file = f"tweets_{tag_to_search}_{start_date}_{end_date}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(tweets_with_replies, f, ensure_ascii=False, indent=2)

        print(f"çˆ¬å–å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ° {output_file}")
        
    finally:
        driver.quit()